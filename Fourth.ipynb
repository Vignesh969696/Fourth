{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e7239-6c15-43fa-93f1-848a0aaaf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "train_path = \"D:/Fourth_project/train_data.csv\"  # change path if needed\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "# Check shape (rows, columns)\n",
    "print(\"Shape of training data:\", df_train.shape)\n",
    "\n",
    "# Preview first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_train.head())\n",
    "\n",
    "# Column names and data types\n",
    "print(\"\\nColumn info:\")\n",
    "df_train.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Unique values for each column (good for categorical features)\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in df_train.columns:\n",
    "    print(f\"{col}: {df_train[col].nunique()} unique values\")\n",
    "\n",
    "# Basic statistics for numerical columns\n",
    "print(\"\\nNumerical column statistics:\")\n",
    "display(df_train.describe())\n",
    "\n",
    "# Value counts for potential target variable(s)\n",
    "# Uncomment and replace 'target_column' with your actual classification target\n",
    "# print(\"\\nTarget distribution:\")\n",
    "# print(df_train['target_column'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787fb3d-dd10-43bf-8b2d-e0cc3f1fd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Optional: display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Step 2: Load Data\n",
    "train_path = \"D:/Fourth_project/train_data.csv\"  # Change if your file is in another folder\n",
    "test_path = \"D:/Fourth_project/test_data.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Test shape:\", df_test.shape)\n",
    "\n",
    "# Quick preview\n",
    "display(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60c40f-14ab-4715-a960-3352c3e55427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Basic Inspection\n",
    "\n",
    "print(\"\\n=== Data Info ===\")\n",
    "df_train.info()\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "print(\"\\n=== Unique Values per Column ===\")\n",
    "for col in df_train.columns:\n",
    "    print(f\"{col}: {df_train[col].nunique()} unique values\")\n",
    "\n",
    "print(\"\\n=== Summary Statistics for Numeric Columns ===\")\n",
    "display(df_train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ec952-34c1-4066-b57c-732dfb65da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read the train and test data\n",
    "train_df = pd.read_csv(\"D:/Fourth_project/train_data.csv\")\n",
    "test_df = pd.read_csv(\"D:/Fourth_project/test_data.csv\")\n",
    "\n",
    "# Features and target\n",
    "X_train = train_df.drop(\"page\", axis=1)  # assuming 'page' is the target\n",
    "y_train = train_df[\"page\"]\n",
    "\n",
    "X_test = test_df.drop(\"page\", axis=1)\n",
    "y_test = test_df[\"page\"]  # Only if labels are available for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed98c5-de77-4038-bdce-ca7aa4fb19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click count per session\n",
    "train_df['click_count'] = train_df.groupby('session_id')['session_id'].transform('count')\n",
    "test_df['click_count'] = test_df.groupby('session_id')['session_id'].transform('count')\n",
    "\n",
    "# Bounce indicator (1 if only one click in session)\n",
    "train_df['is_bounce'] = (train_df['click_count'] == 1).astype(int)\n",
    "test_df['is_bounce'] = (test_df['click_count'] == 1).astype(int)\n",
    "\n",
    "# Last page in session\n",
    "train_df['is_last_page'] = (train_df['order'] == train_df.groupby('session_id')['order'].transform('max')).astype(int)\n",
    "test_df['is_last_page'] = (test_df['order'] == test_df.groupby('session_id')['order'].transform('max')).astype(int)\n",
    "\n",
    "# Merge feature engineering back into X\n",
    "X_train = X_train.assign(\n",
    "    click_count=train_df['click_count'],\n",
    "    is_bounce=train_df['is_bounce'],\n",
    "    is_last_page=train_df['is_last_page']\n",
    ")\n",
    "\n",
    "X_test = X_test.assign(\n",
    "    click_count=test_df['click_count'],\n",
    "    is_bounce=test_df['is_bounce'],\n",
    "    is_last_page=test_df['is_last_page']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b4ed7-7ee3-4107-a083-6f917c2990d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define columns for preprocessing\n",
    "# =========================\n",
    "\n",
    "# Remove leakage column 'page2_clothing_model'\n",
    "categorical_cols = [\"page1_main_category\", \"country\", \"colour\",\n",
    "                    \"location\", \"model_photography\", \"price_2\"]\n",
    "\n",
    "numeric_cols = [\"year\", \"month\", \"day\", \"order\", \"session_id\", \"price\",\n",
    "                \"click_count\", \"is_bounce\", \"is_last_page\"]\n",
    "\n",
    "# Transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae55265-8436-4b5f-9b23-038a4e66ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Basic info\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(train_df.info())\n",
    "print(train_df.describe())\n",
    "\n",
    "# Target distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='page', data=train_df)\n",
    "plt.title('Target Variable Distribution (page)')\n",
    "plt.show()\n",
    "\n",
    "# Numeric feature distributions\n",
    "num_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "train_df[num_cols].hist(figsize=(15,10), bins=20)\n",
    "plt.suptitle('Numeric Feature Distributions')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(train_df[num_cols + ['page']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e4c7d-c3a2-4883-9e80-86a297a00c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline  # <-- use this\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Build pipeline with SMOTE\n",
    "clf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train.drop(\"page2_clothing_model\", axis=1), y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf.predict(X_train.drop(\"page2_clothing_model\", axis=1))\n",
    "y_test_pred = clf.predict(X_test.drop(\"page2_clothing_model\", axis=1))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048b929-5df8-4178-af05-11d317050098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv(\"D:/Fourth_project/train_data.csv\")\n",
    "test_df = pd.read_csv(\"D:/Fourth_project/test_data.csv\")\n",
    "\n",
    "X_train = train_df.drop(\"page\", axis=1)\n",
    "y_train = train_df[\"page\"]\n",
    "X_test = test_df.drop(\"page\", axis=1)\n",
    "y_test = test_df[\"page\"]\n",
    "\n",
    "# Column types\n",
    "numeric_cols = [\"year\", \"month\", \"day\", \"order\", \"session_id\", \"price\"]\n",
    "categorical_cols = [\"page1_main_category\", \"country\", \"colour\",\n",
    "                    \"location\", \"model_photography\", \"price_2\"]\n",
    "\n",
    "# Preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example: Logistic Regression pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "lr_param_grid = {\n",
    "    \"classifier__C\": [0.1, 1],  # smaller grid for speed\n",
    "    \"classifier__solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "lr_search = GridSearchCV(lr_pipeline, lr_param_grid, cv=2, n_jobs=-1)\n",
    "lr_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Logistic Regression params:\", lr_search.best_params_)\n",
    "print(\"Train Accuracy:\", lr_search.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", lr_search.score(X_test, y_test))\n",
    "\n",
    "# Decision Tree pipeline\n",
    "dt_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "dt_param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10],  # smaller depth\n",
    "    \"classifier__min_samples_split\": [5, 10]\n",
    "}\n",
    "\n",
    "dt_search = GridSearchCV(dt_pipeline, dt_param_grid, cv=2, n_jobs=-1)\n",
    "dt_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Decision Tree params:\", dt_search.best_params_)\n",
    "print(\"Train Accuracy:\", dt_search.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", dt_search.score(X_test, y_test))\n",
    "\n",
    "# Random Forest pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=50))  # smaller forest\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10],\n",
    "    \"classifier__min_samples_split\": [5, 10]\n",
    "}\n",
    "\n",
    "rf_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=2, n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest params:\", rf_search.best_params_)\n",
    "print(\"Train Accuracy:\", rf_search.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", rf_search.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ee296-cb41-4509-8ae2-b1e1f4a92b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Read data\n",
    "train_df = pd.read_csv(\"D:/Fourth_project/train_data.csv\")\n",
    "test_df = pd.read_csv(\"D:/Fourth_project/test_data.csv\")\n",
    "\n",
    "X_train = train_df.drop(\"page\", axis=1)\n",
    "y_train = train_df[\"page\"]\n",
    "X_test = test_df.drop(\"page\", axis=1)\n",
    "y_test = test_df[\"page\"]\n",
    "\n",
    "# Column types\n",
    "numeric_cols = [\"year\", \"month\", \"day\", \"order\", \"session_id\", \"price\"]\n",
    "categorical_cols = [\"page1_main_category\", \"country\", \"colour\",\n",
    "                    \"location\", \"model_photography\", \"price_2\"]\n",
    "\n",
    "# Preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Shift target labels for XGBoost\n",
    "y_train_xgb = y_train - 1\n",
    "y_test_xgb = y_test - 1\n",
    "\n",
    "# XGBoost pipeline with SMOTE\n",
    "xgb_pipeline = ImbPipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"classifier\", XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [3, 5],\n",
    "    \"classifier__learning_rate\": [0.1, 0.2]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "xgb_search = GridSearchCV(xgb_pipeline, param_grid, cv=2, n_jobs=-1)\n",
    "xgb_search.fit(X_train, y_train_xgb)\n",
    "\n",
    "# Predictions (shift back to original labels)\n",
    "y_train_pred = xgb_search.predict(X_train) + 1\n",
    "y_test_pred = xgb_search.predict(X_test) + 1\n",
    "\n",
    "# Evaluation\n",
    "print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a870a-2e3d-4195-b63c-28052e4a498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# -----------------------------\n",
    "# Load or train your model here\n",
    "# For demo, we'll train a simple classifier on uploaded CSV\n",
    "# -----------------------------\n",
    "@st.cache_data\n",
    "def train_model(df):\n",
    "    X = df.drop(\"page\", axis=1)\n",
    "    y = df[\"page\"]\n",
    "\n",
    "    numeric_cols = [\"year\", \"month\", \"day\", \"order\", \"session_id\", \"price\"]\n",
    "    categorical_cols = [\"page1_main_category\", \"country\", \"colour\",\n",
    "                        \"location\", \"model_photography\", \"price_2\"]\n",
    "\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_cols),\n",
    "            (\"cat\", categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(max_iter=500))\n",
    "    ])\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "# -----------------------------\n",
    "# Streamlit App\n",
    "# -----------------------------\n",
    "st.title(\"E-Commerce User Behavior Predictor\")\n",
    "\n",
    "# Upload CSV\n",
    "uploaded_file = st.file_uploader(\"Upload your CSV file\", type=[\"csv\"])\n",
    "if uploaded_file:\n",
    "    df = pd.read_csv(uploaded_file)\n",
    "    st.write(\"Uploaded Data:\")\n",
    "    st.dataframe(df.head())\n",
    "\n",
    "    if \"page\" in df.columns:\n",
    "        model = train_model(df)\n",
    "        st.success(\"Model trained on uploaded data!\")\n",
    "\n",
    "        # Predict using entire dataset\n",
    "        if st.button(\"Predict on uploaded data\"):\n",
    "            X = df.drop(\"page\", axis=1)\n",
    "            preds = model.predict(X)\n",
    "            df[\"Predicted Page\"] = preds\n",
    "            st.write(df.head())\n",
    "            st.write(\"Prediction Distribution:\")\n",
    "            st.bar_chart(df[\"Predicted Page\"].value_counts())\n",
    "\n",
    "    # Optional Clustering\n",
    "    if st.checkbox(\"Show K-Means Clustering (2 clusters)\"):\n",
    "        cluster_cols = [\"order\", \"session_id\", \"price\"]  # numeric columns for clustering\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        df['Cluster'] = kmeans.fit_predict(df[cluster_cols])\n",
    "        st.write(df.head())\n",
    "        st.bar_chart(df['Cluster'].value_counts())\n",
    "\n",
    "    # Visualizations\n",
    "    if st.checkbox(\"Show Feature Visualizations\"):\n",
    "        num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "        st.write(\"Histograms for numeric features:\")\n",
    "        for col in num_cols:\n",
    "            fig, ax = plt.subplots()\n",
    "            sns.histplot(df[col], bins=20, kde=False, ax=ax)\n",
    "            ax.set_title(f\"Distribution of {col}\")\n",
    "            st.pyplot(fig)\n",
    "\n",
    "        cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        st.write(\"Bar charts for categorical features:\")\n",
    "        for col in cat_cols:\n",
    "            fig, ax = plt.subplots()\n",
    "            df[col].value_counts().plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f\"Distribution of {col}\")\n",
    "            st.pyplot(fig)\n",
    "\n",
    "# Manual Input Prediction\n",
    "st.header(\"Manual Input Prediction\")\n",
    "st.write(\"Enter feature values to predict page:\")\n",
    "try:\n",
    "    year = st.number_input(\"Year\", value=2008)\n",
    "    month = st.number_input(\"Month\", value=5)\n",
    "    day = st.number_input(\"Day\", value=14)\n",
    "    order = st.number_input(\"Order\", value=1)\n",
    "    session_id = st.number_input(\"Session ID\", value=1)\n",
    "    price = st.number_input(\"Price\", value=50)\n",
    "    page1_main_category = st.number_input(\"Page1 Main Category\", value=1)\n",
    "    country = st.number_input(\"Country\", value=29)\n",
    "    colour = st.number_input(\"Colour\", value=5)\n",
    "    location = st.number_input(\"Location\", value=3)\n",
    "    model_photography = st.number_input(\"Model Photography\", value=1)\n",
    "    price_2 = st.number_input(\"Price 2\", value=1)\n",
    "\n",
    "    input_df = pd.DataFrame([{\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": day,\n",
    "        \"order\": order,\n",
    "        \"session_id\": session_id,\n",
    "        \"price\": price,\n",
    "        \"page1_main_category\": page1_main_category,\n",
    "        \"country\": country,\n",
    "        \"colour\": colour,\n",
    "        \"location\": location,\n",
    "        \"model_photography\": model_photography,\n",
    "        \"price_2\": price_2\n",
    "    }])\n",
    "\n",
    "    if st.button(\"Predict Page for Manual Input\"):\n",
    "        if uploaded_file:\n",
    "            pred = model.predict(input_df)\n",
    "            st.success(f\"Predicted Page: {pred[0]}\")\n",
    "        else:\n",
    "            st.warning(\"Upload CSV first to train the model!\")\n",
    "\n",
    "except Exception as e:\n",
    "    st.error(f\"Error in manual input: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
